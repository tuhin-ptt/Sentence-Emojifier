{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing necessary modules","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport emoji\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-10-07T05:22:24.208902Z","iopub.execute_input":"2021-10-07T05:22:24.209482Z","iopub.status.idle":"2021-10-07T05:22:25.291535Z","shell.execute_reply.started":"2021-10-07T05:22:24.209327Z","shell.execute_reply":"2021-10-07T05:22:25.290507Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Reading pretrained Glove embedding vectors\nVocabulary is created from Glove embedding words","metadata":{}},{"cell_type":"code","source":"vocab = set()\nword2vec_map = {}\nword2Ind = {}\nInd2word = {}\nwith open(\"../input/glove6b50dtxt/glove.6B.50d.txt\", 'r') as f:\n    for line in f:\n        line = line.strip().split()\n        curr_word = line[0]\n        vocab.add(curr_word)\n        word2vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n\n    idx = 1  #zero is used for PAD\n    for word in sorted(vocab):\n        word2Ind[word] = idx\n        Ind2word[idx] = word\n        idx = idx + 1","metadata":{"execution":{"iopub.status.busy":"2021-10-07T05:22:33.513116Z","iopub.execute_input":"2021-10-07T05:22:33.513471Z","iopub.status.idle":"2021-10-07T05:22:45.676972Z","shell.execute_reply.started":"2021-10-07T05:22:33.513438Z","shell.execute_reply":"2021-10-07T05:22:45.676068Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Function to convert emoji number to emoji object","metadata":{}},{"cell_type":"code","source":"def label_to_emoji(label):\n    emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",\n                    \"1\": \":baseball:\",\n                    \"2\": \":smile:\",\n                    \"3\": \":disappointed:\",\n                    \"4\": \":fork_and_knife:\"}\n    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T05:28:47.023833Z","iopub.execute_input":"2021-10-07T05:28:47.024478Z","iopub.status.idle":"2021-10-07T05:28:47.030706Z","shell.execute_reply.started":"2021-10-07T05:28:47.024441Z","shell.execute_reply":"2021-10-07T05:28:47.029649Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Loading dataset","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/emojifydata/emojifier_dataset.csv\")\nX = data['sentence']\ny = data['emoji']\nfor i in range(5):\n    print(X[i], label_to_emoji(y[i]))","metadata":{"id":"dAH9JHab-2bo","outputId":"839ce759-9b11-4f60-c6bb-e2604a3dfb7d","execution":{"iopub.status.busy":"2021-10-07T05:28:50.217957Z","iopub.execute_input":"2021-10-07T05:28:50.218853Z","iopub.status.idle":"2021-10-07T05:28:50.237342Z","shell.execute_reply.started":"2021-10-07T05:28:50.218805Z","shell.execute_reply":"2021-10-07T05:28:50.236647Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Split data into training and testing sets","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T05:36:34.783212Z","iopub.execute_input":"2021-10-07T05:36:34.783579Z","iopub.status.idle":"2021-10-07T05:36:34.791627Z","shell.execute_reply.started":"2021-10-07T05:36:34.783545Z","shell.execute_reply":"2021-10-07T05:36:34.790706Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"max_len = len(max(X_train, key=len).split()) #maximum sentence length in training set\nmax_len","metadata":{"execution":{"iopub.status.busy":"2021-10-07T05:28:54.823026Z","iopub.execute_input":"2021-10-07T05:28:54.823942Z","iopub.status.idle":"2021-10-07T05:28:54.833687Z","shell.execute_reply.started":"2021-10-07T05:28:54.823896Z","shell.execute_reply":"2021-10-07T05:28:54.832735Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Converting sentences to integers","metadata":{}},{"cell_type":"code","source":"def sentences_to_indices(X, word2Ind, max_len):\n    \n    X_indices = [] \n    for i, sentence in enumerate(X):  # loop over training examples\n        sentence_words = sentence.lower().split()\n        sentence_indices = []\n        for word in sentence_words:\n            sentence_indices.append(word2Ind[word])\n        num_pad = max_len - len(sentence_indices)\n        #PADs are added at beginning so that last hidden state of LSTM is more meaningful\n        sentence_indices =  num_pad*[0] + sentence_indices #zero represents PAD\n        X_indices.append(sentence_indices)    \n    return np.array(X_indices)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T05:28:58.403046Z","iopub.execute_input":"2021-10-07T05:28:58.403386Z","iopub.status.idle":"2021-10-07T05:28:58.410654Z","shell.execute_reply.started":"2021-10-07T05:28:58.403338Z","shell.execute_reply":"2021-10-07T05:28:58.409708Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\nX_train_indices = sentences_to_indices(X_train, word2Ind, max_len)\ny_train_oh = to_categorical(y_train, num_classes=5)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T05:29:08.310276Z","iopub.execute_input":"2021-10-07T05:29:08.310530Z","iopub.status.idle":"2021-10-07T05:29:08.321856Z","shell.execute_reply.started":"2021-10-07T05:29:08.310501Z","shell.execute_reply":"2021-10-07T05:29:08.320920Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Model, Input\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout, Activation, Bidirectional","metadata":{"execution":{"iopub.status.busy":"2021-10-07T05:31:55.248454Z","iopub.execute_input":"2021-10-07T05:31:55.248777Z","iopub.status.idle":"2021-10-07T05:31:55.254239Z","shell.execute_reply.started":"2021-10-07T05:31:55.248749Z","shell.execute_reply":"2021-10-07T05:31:55.253331Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Setting pretrained Glove embedding to keras Embedding layer","metadata":{}},{"cell_type":"code","source":"def pretrained_embedding_layer(word2vec_map, word2Ind):\n    '''Keras embedding layer is loaded with pretrained Glove vectors'''\n    vocab_size = len(word2Ind) + 1   # adding 1 to fit Keras embedding (requirement)\n    emb_dim = len(word2vec_map[\"a\"]) \n    \n    emb_matrix = np.zeros((vocab_size, emb_dim))\n    for word, idx in word2Ind.items():\n        emb_matrix[idx, :] = word2vec_map[word]\n\n    embedding_layer = Embedding(vocab_size, emb_dim, trainable = False)\n    # Build the embedding layer, it is required before setting the weights of the embedding layer. \n    embedding_layer.build((None,))\n    # Set the weights of the embedding layer to the embedding matrix. This layer is now pretrained.\n    embedding_layer.set_weights([emb_matrix])\n    \n    return embedding_layer","metadata":{"id":"o56Z8PsJAFSX","execution":{"iopub.status.busy":"2021-10-07T05:31:57.380600Z","iopub.execute_input":"2021-10-07T05:31:57.381422Z","iopub.status.idle":"2021-10-07T05:31:57.390993Z","shell.execute_reply.started":"2021-10-07T05:31:57.381354Z","shell.execute_reply":"2021-10-07T05:31:57.389658Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Defining model","metadata":{}},{"cell_type":"code","source":"def emojify_model(max_len, word2vec_map, word2Ind):\n\n    sentence_indices = Input(shape = (max_len, ), dtype = 'int32')\n    embeddings = pretrained_embedding_layer(word2vec_map, word2Ind)(sentence_indices)\n    X = LSTM(128, return_sequences = True)(embeddings)\n    X = LSTM(128, return_sequences = False)(X) #outputs last hidden state\n    X = Dropout(0.5)(X)\n    X = Dense(5)(X)  #output logits for 5 different emojies\n    output = Activation('softmax')(X)\n    \n    model = Model(inputs = sentence_indices, outputs = output)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","metadata":{"id":"CXzgARFWAHpq","execution":{"iopub.status.busy":"2021-10-07T05:37:16.739024Z","iopub.execute_input":"2021-10-07T05:37:16.739311Z","iopub.status.idle":"2021-10-07T05:37:16.747321Z","shell.execute_reply.started":"2021-10-07T05:37:16.739284Z","shell.execute_reply":"2021-10-07T05:37:16.746412Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model = emojify_model(max_len, word2vec_map, word2Ind)\nmodel.summary()","metadata":{"id":"rk-I_mR1AMDx","execution":{"iopub.status.busy":"2021-10-07T05:38:55.290863Z","iopub.execute_input":"2021-10-07T05:38:55.291342Z","iopub.status.idle":"2021-10-07T05:38:57.087914Z","shell.execute_reply.started":"2021-10-07T05:38:55.291302Z","shell.execute_reply":"2021-10-07T05:38:57.086753Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## Training model with training data","metadata":{}},{"cell_type":"code","source":"model.fit(X_train_indices, y_train_oh, epochs = 50, batch_size = 32, shuffle=True)","metadata":{"id":"z1fhRtAPAOqs","outputId":"0e613872-3ab2-48da-9e5b-b19680f8524b","execution":{"iopub.status.busy":"2021-10-07T05:38:57.336059Z","iopub.execute_input":"2021-10-07T05:38:57.336397Z","iopub.status.idle":"2021-10-07T05:39:07.307058Z","shell.execute_reply.started":"2021-10-07T05:38:57.336342Z","shell.execute_reply":"2021-10-07T05:39:07.306272Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating model performance","metadata":{}},{"cell_type":"code","source":"X_test_indices = sentences_to_indices(X_test, word2Ind, max_len)\ny_test_oh = to_categorical(y_test, num_classes=5)\nloss, accuracy = model.evaluate(X_test_indices, y_test_oh)\nprint()\nprint(\"Test accuracy = \", accuracy)","metadata":{"id":"zv8u_EejARBp","execution":{"iopub.status.busy":"2021-10-07T05:39:15.207466Z","iopub.execute_input":"2021-10-07T05:39:15.207821Z","iopub.status.idle":"2021-10-07T05:39:15.285806Z","shell.execute_reply.started":"2021-10-07T05:39:15.207769Z","shell.execute_reply":"2021-10-07T05:39:15.284650Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"sentence = 'i am not feeling happy'\ntest_sentence = np.array([sentence])\ntest_sentence_indices = sentences_to_indices(test_sentence, word2Ind, max_len)\ntest_pred = model.predict(test_sentence_indices)\nprint(test_sentence[0] +' '+  label_to_emoji(np.argmax(test_pred)))","metadata":{"id":"_mdc0u8gATMd","outputId":"69efc692-29d0-48d6-e2bd-1173320f9af7","execution":{"iopub.status.busy":"2021-10-07T05:42:55.436203Z","iopub.execute_input":"2021-10-07T05:42:55.436862Z","iopub.status.idle":"2021-10-07T05:42:55.492448Z","shell.execute_reply.started":"2021-10-07T05:42:55.436815Z","shell.execute_reply":"2021-10-07T05:42:55.491318Z"},"trusted":true},"execution_count":40,"outputs":[]}]}